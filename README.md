# Logit Lens для VLM

В данном эксперименте я использовал llava-1.5-7b-h и готовую имплементацию Logit Lens.  
The BibTeX entry for citing this work can be found [here](citation.bib).  
Для самостоятельного анализа можно ознакомиться с файлами в директории [result](result).
## Чтобы самостоятельно сгенерировать HTML файлы, следуйте следующей инструкции из [данного репозитория](https://github.com/clemneo/llava-interp/tree/main).

# **Отчет**
---
***Logit Lens*** - это способ изучения промежуточных представлений предсказаний на разных этапах(слоях) генерации ответа, то есть таким образом можно наблюдать, как эволюционируют предсказания модели на каждом слое.
---
**Более формально:**
$l \in$ {1,...,L} - номер слоя.

$t_1, ... , t_n \in V$ - последовательность токенов

$h_l^i \in R^d$ - скрытое состояние токена $t_i$ на слое $l$

Тогда логиты финального распределения задаются выражением:  
$p(t_{n+1} | t_1,...,t_n) = [logit_1,...,logit_{|V|}] = W_U \cdot LayerNorm_L(h_L^n)$

При применении Logit Lens к более ранним скрытым состояниям $h_l^i$ "дешифровка" логитов выглядит следующим образом:

$[logit_1^l,...,logit_{|V|}^l] = W_U \cdot LayerNorm_L(h_l^n)$

---

## В ходе эксперимента были обнаружены следующие паттерны:

- Конкретные детали объектов:
Декодированные токены часто соответствовали более конкретным концепциям, чем на уровне объекта. Например, токены могут не только называть объект на фото, но и описывать его свойства.


- Языковая специфика:  
Токены иногда соответствовали концепциям на других языках.  
В процессе обучения модель может "запомнить" паттерны или токены из разных языков, даже если основная задача связана с одним языком (например, английским). Эти токены могут случайно активироваться в промежуточных слоях.

